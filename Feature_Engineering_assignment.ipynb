{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCE_qPKLejxJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering"
      ],
      "metadata": {
        "id": "SyAFUZ2Me3De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1.  What is a parameter?\n",
        "\n",
        "    A parameter is a value that is used to customize the behavior of a function,\n",
        "    method, or system. It serves as an input that influences how\n",
        "    something operates.\n",
        "\n",
        "    1.Mathematics:\n",
        "\n",
        "A parameter is a constant in an equation that defines specific characteristics\n",
        "of a function or system.\n",
        "\n",
        "Example: In the equation of a line, y=mx+b, m (slope) and b (y-intercept) are\n",
        " parameters.\n",
        "\n",
        "   2.Programming:\n",
        "\n",
        "A parameter is a variable passed to a function to control its behavior.\n",
        "\n",
        "Example (Python):\n",
        "\n",
        "def greet(name):\n",
        "    print(\"Hello, \" + name)\n",
        "greet(\"Alice\")  # \"Alice\" is the argument, \"name\" is the parameter.\n",
        "\n",
        "    3.Science & Engineering:\n",
        "\n",
        "A parameter represents a measurable factor that defines a system's state or behavior.\n",
        "Example: Temperature and pressure in a thermodynamic system.\n",
        "\n",
        "    4.Statistics & Machine Learning:\n",
        "\n",
        "A parameter is a value that describes a population or a model\n",
        " (e.g., weights in a neural network).\n",
        "Example: In a normal distribution, the mean μ) and standard deviation (σ)\n",
        "are parameters.\n",
        "\n",
        "2.  What is correlation?\n",
        "\n",
        "    Correlation is a statistical measure that describes the relationship between\n",
        "    two variables. It shows how one variable changes in response to another.\n",
        "\n",
        "    # Positive correlation → Both variables increase or decrease together.\n",
        "\n",
        "    # Negative correlation → One variable increases while the other decreases.\n",
        "\n",
        "    # No correlation → No clear relationship between the variables.\n",
        "\n",
        "\n",
        "    What does negative correlation mean?\n",
        "\n",
        "    A negative correlation (or inverse correlation) means that as one\n",
        "    variable increases, the other decreases.\n",
        "\n",
        "# Examples of Negative Correlation:\n",
        "1.Temperature & Hot Coffee Sales → As temperature rises, hot coffee sales drop.\n",
        "2.Exercise & Body Fat Percentage → More exercise usually leads to lower body fat.\n",
        "3.Demand & Price (Economics) → When the price of a product rises, demand typically falls.\n",
        "\n",
        "# Measuring Correlation:\n",
        "The correlation coefficient (r) ranges from -1 to 1:\n",
        "\n",
        "1. r = -1 → Perfect negative correlation (one variable always decreases\n",
        "                                         when the other increases).\n",
        "\n",
        "2. r = 0 → No correlation.\n",
        "\n",
        "3. r = 1 → Perfect positive correlation (both increase/decrease together).\n",
        "\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "      Machine Learning (ML) is a branch of artificial intelligence (AI) that\n",
        "      enables computers to learn from data and make decisions or predictions\n",
        "      without being explicitly programmed. It allows systems to recognize\n",
        "      patterns, improve performance over time, and adapt to new inputs.\n",
        "\n",
        "# Main Components of Machine Learning\n",
        "\n",
        " 1. Data\n",
        "\n",
        "The foundation of ML. It can be structured (tables, databases) or\n",
        "unstructured (text, images, videos).\n",
        "Example: Customer purchase history, medical records, or stock prices.\n",
        "\n",
        " 2. Features (Independent Variables)\n",
        "\n",
        "The measurable properties or characteristics used to make predictions.\n",
        "Example: In predicting house prices, features could be square footage,\n",
        "number of bedrooms, and location.\n",
        "\n",
        "3. Model\n",
        "\n",
        "The mathematical representation of patterns in the data.\n",
        "Example: A decision tree, neural network, or linear regression model.\n",
        "\n",
        "4. Algorithm\n",
        "\n",
        "The method used to train the model by finding patterns in the data.\n",
        "\n",
        "Example:\n",
        "Supervised Learning (e.g., Decision Trees, SVM, Neural Networks)\n",
        "Unsupervised Learning (e.g., K-Means, PCA)\n",
        "Reinforcement Learning (e.g., Q-Learning, Deep Q-Networks)\n",
        "\n",
        " 5. Training Process\n",
        "\n",
        "The process of feeding data into the model so it can learn patterns.\n",
        "This involves adjusting parameters (e.g., weights in a neural network)\n",
        "to minimize error.\n",
        "\n",
        "6. Loss Function & Optimization\n",
        "\n",
        "Measures how far the model's predictions are from actual values.\n",
        "Optimization techniques (e.g., Gradient Descent) adjust the model to reduce errors.\n",
        "\n",
        " 7. Evaluation & Validation\n",
        "\n",
        "Testing the trained model on unseen data to measure its accuracy and performance.\n",
        "Common techniques: Train-Test Split, Cross-Validation.\n",
        "\n",
        " 8. Deployment\n",
        "\n",
        "Once a model performs well, it is deployed for real-world use,\n",
        "such as in recommendation systems or self-driving cars.\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "The loss value measures how far a model's predictions are from the actual values.\n",
        "It is a key indicator of a model's performance. A lower loss value means the\n",
        "model is making better predictions, while a higher loss value indicates poor\n",
        "performance.\n",
        "\n",
        " # How Loss Helps Determine Model Quality\n",
        "\n",
        " 1. Measures Model Error\n",
        "\n",
        "   1. A high loss means the model is making incorrect predictions.\n",
        "   2. A low loss suggests the model is learning patterns well.\n",
        "\n",
        " 2. Guides Model Optimization\n",
        "\n",
        "   1. During training, optimization algorithms (like Gradient Descent)\n",
        "      adjust the model's parameters to minimize the loss.\n",
        "   2. If the loss does not decrease, the model might be stuck or not\n",
        "      learning properly.\n",
        "\n",
        " 3. Prevents Overfitting or Underfitting\n",
        "\n",
        "   1. Overfitting: The loss on training data is low, but on test data,\n",
        "      it's high → The model memorizes training data but fails to generalize.\n",
        "   2. Underfitting: The loss is high for both training and test data →\n",
        "      The model is too simple and hasn’t learned enough from the data.\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "\n",
        "   #  Continuous Variables\n",
        "        A continuous variable can take any numerical value within a range.\n",
        "        It is measurable and can have decimal or fractional values.\n",
        "\n",
        "Examples:\n",
        "   > Height (e.g., 5.8 feet, 170.3 cm)\n",
        "   > Weight (e.g., 65.2 kg, 150.5 lbs)\n",
        "   > Temperature (e.g., 37.5°C, 98.6°F)\n",
        "   > Time (e.g., 2.35 hours, 15.7 minutes)\n",
        "\n",
        "Key Characteristics:\n",
        "✔ Infinite possible values\n",
        "✔ Can be measured (not just counted)\n",
        "✔ Can have decimal points\n",
        "\n",
        "#  Categorical Variables\n",
        "A categorical variable represents groups or categories that do not have a\n",
        "natural numerical order.\n",
        "\n",
        "Types of Categorical Variables:\n",
        "🔹 Nominal Variables (No specific order)\n",
        "    Example: Colors (Red, Blue, Green), Cities (New York, Paris, Tokyo)\n",
        "\n",
        "🔹 Ordinal Variables (Have a meaningful order)\n",
        "    Example: Education Level (High School < Bachelor’s < Master’s < PhD)\n",
        "    Example: Customer Ratings (Poor < Fair < Good < Excellent)\n",
        "\n",
        "Key Characteristics:\n",
        "✔ Represent labels or categories\n",
        "✔ Cannot be measured, only counted\n",
        "✔ Ordinal variables have a meaningful order\n",
        "\n",
        "6.  How do we handle categorical variables in Machine Learning? What are the\n",
        "common techniques?\n",
        "\n",
        "        Machine learning models work with numerical data, so categorical\n",
        "        variables need to be converted into numerical format before training.\n",
        "        There are several techniques to do this, depending on the type of\n",
        "        categorical variable (nominal or ordinal).\n",
        "\n",
        "\n",
        "   # Common Techniques for Handling Categorical Variables\n",
        "\n",
        "   # 1. Label Encoding (For Ordinal Variables)\n",
        "\n",
        "     > Assigns a unique integer to each category.\n",
        "     > Works well when categories have a meaningful order.\n",
        "\n",
        "     Example:\n",
        " Education Level → (High School < Bachelor's < Master's < PhD)\n",
        "\n",
        "   # 2. One-Hot Encoding (OHE) (For Nominal Variables)\n",
        "\n",
        "     > Converts each category into a binary (0 or 1) column.\n",
        "     > Suitable for unordered categories.\n",
        "\n",
        "Example:\n",
        " Cities: {New York, Paris, Tokyo}\n",
        "\n",
        "  # 3. Ordinal Encoding (For Ordered Categorical Variables)\n",
        "\n",
        "    > Similar to Label Encoding but ensures the order is maintained.\n",
        "    > Suitable for ordinal variables (e.g., customer satisfaction:\n",
        "                                      Bad < Average < Good < Excellent\n",
        "\n",
        "\n",
        "  # 4. Frequency Encoding (For High-Cardinality Variables)\n",
        "\n",
        "    > Replaces categories with their occurrence count.\n",
        "    > Works well when category frequency is meaningful.\n",
        "\n",
        "Example:\n",
        " Car Brands in a dataset.\n",
        "\n",
        "  # 5. Target Encoding (Mean Encoding) (For High-Cardinality Variables ✅)\n",
        "\n",
        "    > Replaces categories with the mean of the target variable.\n",
        "    > Used in supervised learning problems.\n",
        "\n",
        "Example:\n",
        " Predicting loan approval (1 = Approved, 0 = Rejected)\n",
        "\n",
        "\n",
        "7.  What do you mean by training and testing a dataset?\n",
        "\n",
        " # Training Dataset 📚\n",
        "\n",
        "  > The training dataset is used to teach the model.\n",
        "  > The model learns patterns, relationships, and trends from this data.\n",
        "  > It adjusts parameters (e.g., weights in a neural network) to minimize errors.\n",
        "\n",
        "✅ Example:\n",
        "If you're training a model to predict house prices, the training dataset would\n",
        "include historical data with features like square footage, number of bedrooms,\n",
        "and actual house prices.\n",
        "\n",
        " # Testing Dataset 🎯\n",
        "\n",
        "  > The testing dataset is used to evaluate the model’s performance.\n",
        "  > It contains new, unseen data that the model has never encountered before.\n",
        "  > The goal is to check if the model can make accurate predictions on fresh inputs.\n",
        "\n",
        "✅ Example:\n",
        "Once trained, the house price model is tested on new houses not included in\n",
        "training. If it predicts well, the model is considered reliable.\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "      sklearn.preprocessing is a module in Scikit-Learn (sklearn) that provides\n",
        "      various functions to transform raw data into a format suitable for machine\n",
        "      learning models.\n",
        "\n",
        "Since ML models work best with numerical, normalized, and standardized data,\n",
        "sklearn.preprocessing helps by:\n",
        "\n",
        "✔ Handling missing values\n",
        "✔ Scaling and normalizing data\n",
        "✔ Encoding categorical variables\n",
        "✔ Transforming feature distributions.\n",
        "\n",
        "\n",
        "9. What is a Test set?\n",
        "\n",
        "       A test set is a portion of the dataset that is used to evaluate a machine\n",
        "      learning model after training. It consists of unseen data that was not\n",
        "      used during training to check how well the model generalizes to new inputs.\n",
        "\n",
        "# Key Points About the Test Set:\n",
        "✅ Purpose: To measure the model's real-world performance.\n",
        "✅ Contains: Data that the model has never seen before.\n",
        "✅ Typical Size: 10%–30% of the total dataset (commonly 20%).\n",
        "✅ Usage: Only used once after training is complete\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "      We use train_test_split from sklearn.model_selection to split data into training\n",
        "      and test sets before fitting a machine learning model.\n",
        "\n",
        "🔹 Example: Train-Test Split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset (features: X, target labels: y)\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
        "\n",
        "# Split data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Set X:\", X_train)\n",
        "print(\"Test Set X:\", X_test)\n",
        "✅ Here:\n",
        "\n",
        "80% of data goes to training (X_train, y_train).\n",
        "\n",
        "20% of data goes to testing (X_test, y_test).\n",
        "\n",
        "random_state=42 ensures consistent results across runs.\n",
        "\n",
        "🔹 Example: Train-Test-Validation Split\n",
        "For better model tuning, we also use a validation set (Train: 70%, Validation: 15%, Test: 15%).\n",
        "\n",
        "python\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Validation set size:\", len(X_val))\n",
        "print(\"Test set size:\", len(X_test))\n",
        "\n",
        "# How Do You Approach a Machine Learning Problem?\n",
        "\n",
        "Step\tDescription\n",
        "1. Define the Problem -\tUnderstand the goal\n",
        "2. Collect Data\t- Gather & explore dataset\n",
        "3. Preprocess Data - Handle missing values, scale, encode\n",
        "4. Split Data\t- Train-Test-Validation\n",
        "5. Train a Model - Choose & fit ML algorithm\n",
        "6. Evaluate Performance\t- Use metrics like accuracy, RMSE\n",
        "7. Improve & Deploy\t- Tune hyperparameters, deploy online\n",
        "\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "        Exploratory Data Analysis (EDA) is a crucial step in machine learning\n",
        "        where we analyze, visualize, and preprocess the dataset before training\n",
        "        a model. Skipping EDA can lead to poor model performance, incorrect\n",
        "        predictions, or misleading results.\n",
        "\n",
        "        Reason                        Benefit\n",
        "        Understand data structure     Avoid errors in model selection\n",
        "        Handle missing values         Prevent biased models\n",
        "        Detect & fix outliers         Improve accuracy\n",
        "        Check correlations            Reduce redundancy\n",
        "        Detect class imbalance\t      Prevent bias in classification\n",
        "        Select right model\t          Improve performance\n",
        "        Feature engineering\t          Boost predictive power\n",
        "\n",
        "# 12 & 13 Questions are the same to question no. 2 which is already answerred.\n",
        "\n",
        "14.  How can you find correlation between variables in Python?\n",
        "\n",
        "          In Python, we can use Pandas and Seaborn to calculate and visualize\n",
        "          correlations between variables.\n",
        "\n",
        "          # 1️⃣ Using corr() from Pandas\n",
        "\n",
        "           The .corr() method computes the Pearson correlation coefficient by default.\n",
        "\n",
        "🔹 Example: Compute Correlation Matrix\n",
        "\n",
        "python\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = {'Study_Hours': [2, 3, 4, 5, 6, 7, 8],\n",
        "        'Exam_Score': [50, 55, 65, 70, 75, 80, 90],\n",
        "        'Sleep_Hours': [8, 7.5, 7, 6.5, 6, 5.5, 5]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n",
        "✅ This will return:\n",
        "\n",
        "markdown\n",
        "\n",
        "             Study_Hours  Exam_Score  Sleep_Hours\n",
        "Study_Hours     1.000000    0.987829   -0.987829\n",
        "Exam_Score      0.987829    1.000000   -0.998611\n",
        "Sleep_Hours    -0.987829   -0.998611    1.000000\n",
        "\n",
        "  Interpretation:\n",
        "\n",
        "Study_Hours & Exam_Score → Strong Positive Correlation (0.98)\n",
        "\n",
        "Sleep_Hours & Exam_Score → Strong Negative Correlation (-0.99)\n",
        "\n",
        "    # 2️⃣ Using Seaborn Heatmap for Visualization\n",
        "\n",
        " A heatmap makes it easy to visualize correlation patterns.\n",
        "\n",
        "🔹 Example: Correlation Heatmap\n",
        "\n",
        "python\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "✅ What this does:\n",
        "\n",
        "Uses color gradients to show correlation strength.\n",
        "\n",
        "annot=True → Displays correlation values inside the heatmap.\n",
        "\n",
        "cmap='coolwarm' → Colors for positive/negative correlations\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation\n",
        "    with an example?\n",
        "\n",
        "      Causation (also called causality) means that one event directly causes\n",
        "      another event to happen. In other words, a change in one variable results\n",
        "      in a change in another variable.\n",
        "\n",
        "✅ Example of Causation:\n",
        "\n",
        "> More hours of studying → Higher exam scores\n",
        "   > Here, increasing study time directly leads to better scores.\n",
        "\n",
        "# Difference Between Correlation and Causation\n",
        "\n",
        "Aspect\t        Correlation \t                      Causation\n",
        "Definition\t    Relationship between two variables  One variable directly affects another\n",
        "                (they move together)\n",
        "Direction\t      No proof of cause-effect\t          Clearly establishes cause-effect\n",
        "Mathematical    Pearson/Spearman correlation        Controlled experiments,\n",
        "Measure         coefficient (r)                     statistical tests\n",
        "Example\t        Ice cream sales & drowning deaths   Smoking causes lung cancer\n",
        "                are correlated but do not cause\n",
        "                each other\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers?\n",
        "    Explain each with an example.\n",
        "\n",
        "      An optimizer is an algorithm used to update the model's parameters\n",
        "      (weights and biases) to minimize the loss function and improve the model’s\n",
        "      accuracy.\n",
        "\n",
        "    # Types\n",
        "  ️  #  Gradient Descent-based Optimizers\n",
        "        These optimizers update the weights by moving in the direction of the\n",
        "        negative gradient of the loss function.\n",
        "\n",
        "  1. Batch Gradient Descent (BGD)\n",
        "    > Updates weights after processing the entire dataset.\n",
        "    > Slow but gives stable convergence.\n",
        "    > Not ideal for large datasets.\n",
        "Example:\n",
        "python\n",
        "# Pseudocode for Batch Gradient Descent\n",
        "for epoch in range(epochs):\n",
        "    gradients = compute_gradient(loss_function, weights)\n",
        "    weights = weights - learning_rate * gradients\n",
        "     Pros: More stable updates.\n",
        "\n",
        "  2. Stochastic Gradient Descent (SGD)\n",
        "    > Updates weights after every single data point (instead of the full dataset).\n",
        "    > Faster but has high variance (less stable).\n",
        "    > Used in real-time learning applications\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "Pros: Works well with large datasets.\n",
        "\n",
        "  3. Mini-Batch Gradient Descent.\n",
        "    > A compromise between BGD and SGD.\n",
        "    > Updates weights after processing a small batch of data points.\n",
        "    > Balances stability & efficiency.\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "Pros: Faster than BGD, more stable than SGD.\n",
        "\n",
        "# Adaptive Learning Rate Optimizers\n",
        "These optimizers automatically adjust the learning rate to improve training.\n",
        "\n",
        "🔹 (i) Momentum\n",
        "   > Uses past gradients to smooth updates.\n",
        "   > Helps in escaping local minima.\n",
        "\n",
        "Example:\n",
        "python\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "✅ Pros: Faster convergence\n",
        "\n",
        "  (ii) RMSprop (Root Mean Square Propagation)\n",
        "   > Adjusts learning rate individually for each parameter.\n",
        "   > Helps in handling vanishing gradients.\n",
        "\n",
        "Example:\n",
        "python\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "optimizer = RMSprop(learning_rate=0.001)\n",
        "✅ Pros: Works well in recurrent neural networks (RNNs)\n",
        "\n",
        " (iii) Adam (Adaptive Moment Estimation)\n",
        "   > Combines Momentum + RMSprop → Adjusts learning rate based on past updates.\n",
        "   > Most widely used optimizer in deep learning.\n",
        "\n",
        "Example:\n",
        "python\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "✅ Pros: Fast convergence, works well on most problems\n",
        "\n",
        "  (iv) AdaGrad (Adaptive Gradient Algorithm)\n",
        "    > Reduces the learning rate for frequently updated weights.\n",
        "    > Good for sparse data (e.g., NLP tasks).\n",
        "\n",
        "Example:\n",
        "python\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "optimizer = Adagrad(learning_rate=0.01)\n",
        "✅ Pros: Good for NLP & sparse data.\n",
        "\n",
        "  v) AdaDelta\n",
        "    > Improves AdaGrad by preventing the learning rate from becoming too small.\n",
        "\n",
        "Example:\n",
        "python\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "optimizer = Adadelta(learning_rate=1.0)\n",
        "✅ Pros: No need to set a learning rate manually\n",
        "\n",
        "17. What is sklearn.linear_model ?\n",
        "\n",
        "       sklearn.linear_model is a module in Scikit-Learn that provides linear\n",
        "       models for regression and classification tasks. These models assume a\n",
        "       linear relationship between input features and the target variable.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "      The model.fit(X, y) function is used to train (fit) a machine learning\n",
        "      model using the provided dataset. It learns patterns from the input data\n",
        "      X and maps them to the output labels y.\n",
        "\n",
        "   # How model.fit() Works\n",
        "   > Takes input features (X) and target labels (y).\n",
        "   > Computes optimal parameters (weights & biases).\n",
        "   > Finds patterns and learns from the data.⃣\n",
        "   > Stores the trained model for making predictions later (model.predict()).\n",
        "\n",
        "   Required Arguments for model.fit()\n",
        "\n",
        "Argument\t         Description\t                   Example\n",
        "X\t                 Input features                  X = [[1], [2], [3]]\n",
        "                  (Independent variables)\n",
        "y\t                 Target variable                 y = [2, 4, 6]\n",
        "                   (Dependent variable)\n",
        "\n",
        "# Example: Training a Linear Regression Model\n",
        "\n",
        "python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample Data\n",
        "X = [[1], [2], [3], [4], [5]]  # Feature (e.g., study hours)\n",
        "y = [2, 4, 6, 8, 10]  # Target (e.g., exam scores)\n",
        "\n",
        "# Create Model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the Model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "print(model.predict([[6]]))  # Output: [12]\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "   model.predict(X_new) is used to make predictions using a trained model.\n",
        "   After fitting (model.fit()), this function takes new input data (X_new) and\n",
        "   returns the predicted output.\n",
        "\n",
        "   # How model.predict() Works\n",
        "    > Takes new input data (X_new).\n",
        "    > Uses the trained model to calculate predictions.⃣\n",
        "    > Returns predicted values based on learned patterns.\n",
        "\n",
        "     Required Argument for predict()\n",
        "Argument\t               Description\t                     Example\n",
        "X_new\t                   New input data                    X_new = [[6]]\n",
        "                         (same features as training data)\n",
        "\n",
        "# Example: Using model.predict() for Regression\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample Data (Training)\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Create & Train Model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make Predictions\n",
        "X_new = [[6], [7]]  # New data points\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "print(predictions)  # Output: [12, 14]\n",
        "\n",
        "🔹 The model learns y = 2x and correctly predicts y = 12 for x = 6.\n",
        "\n",
        "\n",
        "# Example: Using predict() for Classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Sample Data\n",
        "X = [[20], [25], [30], [35], [40]]  # Age\n",
        "y = [0, 0, 1, 1, 1]  # 0 = No Disease, 1 = Disease\n",
        "\n",
        "# Train Model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict for new data\n",
        "X_new = [[28], [38]]\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "print(predictions)  # Output: [0, 1]\n",
        "🔹 The model predicts age 28 → No Disease (0), age 38 → Disease (1)\n",
        "\n",
        "# Question No.5 & Question No.20 are the same.\n",
        "\n",
        "21.What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "     Feature scaling is a data preprocessing technique that transforms numerical\n",
        "     features into a standard range (e.g., [0,1] or mean=0, std=1).\n",
        "     This ensures that all features contribute equally to the model and improves\n",
        "     its performance.\n",
        "\n",
        "     # Types of Feature Scaling Techniques\n",
        "\n",
        "     1. Min-Max Scaling (Normalization)\n",
        "     > Scales values between 0 and 1 (or any custom range).\n",
        "\n",
        "Example in Python:\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[100], [200], [300], [400], [500]])  # Unscaled feature\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)  # Output: [[0. ] [0.25] [0.5] [0.75] [1. ]]\n",
        "\n",
        "   2. Standardization (Z-Score Normalization)\n",
        "  > Transforms data to zero mean and unit variance.\n",
        "\n",
        "Example in Python:\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(X)\n",
        "\n",
        "print(X_standardized)  # Mean ≈ 0, Std ≈ 1\n",
        "\n",
        "  3. Robust Scaling\n",
        "  > Uses median and interquartile range (IQR) to handle outliers.\n",
        "\n",
        "Example in Python:\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_robust = scaler.fit_transform(X)\n",
        "\n",
        "print(X_robust)  # Less affected by outliers\n",
        "\n",
        "22.  How do we perform scaling in Python?\n",
        "\n",
        "    #  Feature Scaling on a Real Dataset (Pandas + Scikit-Learn)\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load dataset\n",
        "data = pd.DataFrame({\n",
        "    'Age': [25, 35, 45, 55, 65],\n",
        "    'Salary': [30000, 50000, 70000, 90000, 110000]\n",
        "})\n",
        "\n",
        "# Apply Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
        "\n",
        "print(data_scaled)\n",
        "\n",
        "# Question no 8 & 23 are the same\n",
        "\n",
        "# Question no 10 & 24 are the same.\n",
        "\n",
        "25. Explain data encoding?\n",
        "\n",
        "      Data encoding is the process of converting categorical variables\n",
        "       (text or labels) into a numerical format so that machine learning models\n",
        "       can understand them. Most ML models work with numbers, not text,\n",
        "       so encoding is essential.\n",
        "\n",
        "    # Why is Data Encoding Important?⃣\n",
        "      1. Machine learning models require numerical input⃣\n",
        "      2. Prevents errors when using categorical feature\n",
        "      3. Improves model accuracy by correctly representing categorical data⃣\n",
        "      4. Some models (like Decision Trees) can work with categorical data,\n",
        "        but others (like Logistic Regression, SVM, or Neural Networks) need encoding\n",
        "\n",
        " # Types of Data Encoding Techniques\n",
        "\n",
        " 1.Label Encoding (Ordinal Encoding)\n",
        "  > Converts each category into a unique integer\n",
        "    (e.g., [\"Red\", \"Green\", \"Blue\"] → [0, 1, 2]).\n",
        "  > Best for: Ordinal data (categories with a meaningful order,\n",
        "    like \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "🔹 Python Example:\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = ['Low', 'Medium', 'High', 'Medium', 'Low']\n",
        "encoder = LabelEncoder()\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "print(encoded_data)  # Output: [1 2 0 2 1]\n",
        "\n",
        "2.One-Hot Encoding (OHE)\n",
        "\n",
        "> Converts categorical variables into binary (0/1) columns.\n",
        "> Best for: Nominal data (categories with no order, like \"Dog\", \"Cat\", \"Bird\").\n",
        "\n",
        "Python Code:\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']})\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_data = encoder.fit_transform(data[['Color']])\n",
        "\n",
        "print(encoded_data)\n",
        "\n",
        "3. Ordinal Encoding\n",
        " > Similar to Label Encoding but maintains order.\n",
        " > Best for: Ordered categories like [\"Beginner\", \"Intermediate\", \"Expert\"] → [0, 1, 2].\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "data = [['Beginner'], ['Intermediate'], ['Expert']]\n",
        "encoder = OrdinalEncoder(categories=[['Beginner', 'Intermediate', 'Expert']])\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "print(encoded_data)  # Output: [[0] [1] [2]]\n",
        "\n",
        "4. Binary Encoding\n",
        "  > Converts categories into binary format and stores as separate columns.\n",
        "  > Best for: Large categorical datasets (better than One-Hot Encoding).\n",
        "\n",
        "  Python Code (Using category_encoders Library):\n",
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({'Category': ['A', 'B', 'C', 'A']})\n",
        "encoder = ce.BinaryEncoder(cols=['Category'])\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "print(encoded_data)\n",
        "\n",
        "5. Target Encoding (Mean Encoding)\n",
        "  > Replaces categories with the mean of the target variable.\n",
        "  > Best for: High-cardinality categorical variables (many unique values).\n",
        "\n",
        "  import category_encoders as ce\n",
        "\n",
        "data = pd.DataFrame({'City': ['NYC', 'LA', 'SF', 'LA'], 'Sales': [50, 45, 55, 46]})\n",
        "encoder = ce.TargetEncoder(cols=['City'])\n",
        "encoded_data = encoder.fit_transform(data['City'], data['Sales'])\n",
        "\n",
        "print(encoded_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w8VHU824e5nB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}